\documentclass[12pt, a4paper]{article}

% If you can't see cyrillic letters in R-studio choose
% File-Reopen with encoding
% utf8 is the preferred encoding


\input{title_bor_utf8_knitr}
\input{emetrix_preamble}


\usepackage{minted}

\usepackage[bibencoding = auto, backend = biber,
sorting = none]{biblatex}

\addbibresource{mlearn_pro.bib}

\def \RR{\mathbb{R}}
\def \cN{\mathcal{N}}

\title{Заметки к семинарам по машинному обучению}
\author{Винни-Пух}
\date{\today}


% делаем короче интервал в списках
\setlength{\itemsep}{0pt}
\setlength{\parskip}{0pt}
\setlength{\parsep}{0pt}


\DeclareMathOperator{\Med}{Med}


\usepackage{answers}

%\newtheorem{problem}{Задача}
%\numberwithin{problem}{section}

\Newassociation{sol}{solution}{solution_file}
% sol --- имя окружения внутри задач
% solution --- имя окружения внутри solution_file
% solution_file --- имя файла в который будет идти запись решений
% можно изменить далее по ходу
\Opensolutionfile{solution_file}[all_solutions]
% в квадратных скобках фактическое имя файла



% магия для автоматических гиперссылок задача-решение
\newlist{myenum}{enumerate}{3}
% \newcounter{problem}[chapter] % нумерация задач внутри глав
\newcounter{problem}[section]

\newenvironment{problem}%
{%
\refstepcounter{problem}%
%  hyperlink to solution
     \hypertarget{problem:{\thesection.\theproblem}}{} % нумерация внутри глав
     % \hypertarget{problem:{\theproblem}}{}
     \Writetofile{solution_file}{\protect\hypertarget{soln:\thesection.\theproblem}{}}
     %\Writetofile{solution_file}{\protect\hypertarget{soln:\theproblem}{}}
     \begin{myenum}[label=\bfseries\protect\hyperlink{soln:\thesection.\theproblem}{\thesection.\theproblem},ref=\thesection.\theproblem]
     % \begin{myenum}[label=\bfseries\protect\hyperlink{soln:\theproblem}{\theproblem},ref=\theproblem]
     \item%
    }%
    {%
    \end{myenum}}
% для гиперссылок обратно надо переопределять окружение
% это происходит непосредственно перед подключением файла с решениями




\begin{document}

% \maketitle % ставим сюда название, автора и время создания

\section{Дифференциал}


Минитеория:

\begin{enumerate}
\item $d(XY) = dX \cdot Y +  X \cdot dY$
\item $dA = 0$
\item $d(X') = dX'$
\item $d\det X = \tr (...) $
\end{enumerate}



\begin{problem}
    Вспомним дифференциал :)
    \begin{enumerate}
        \item Известно, что $f(x) = x^2 + 3x$. Найдите $f'(x)$ и $df$. Чему равен $df$ в точке $x=5$ при $dx=0.1$? % было «чему равен dx» - опечатка
        \item Известно, что $f(x_1, x_2)=x_1^2 + 3x_1x_2^3$. Найдите $df$. Чему равен $df$ в точке $x_1=-2$, $x_2=1$ при $dx_1=0.1$ и $dx_2=-0.1$?
        \item Известно, что $F=\begin{pmatrix}
                5 & 6x_1 \\
                x_1x_2 & x_1^2x_2 \\
            \end{pmatrix}$. Найдите $dF$.
        \item Известно, что $F=\begin{pmatrix}
                7 & 8 & 9 \\
                2 & -1 & -2 \\
            \end{pmatrix}$. Найдите $dF$.
        \item Матрица $F$ имеет размер $2\times 2$, в строке $i$ столбце $j$ у неё находится элемент $f_{ij}$.
            Выпишите выражение $\tr(F'dF)$ в явном виде без матриц.
    \end{enumerate}
\begin{sol}
\begin{enumerate}
\item $f'(x) = 2x + 3$, $df = 2xdx + 3dx$, $df = 1.3$
\item $df = 2 x_1 d x_1 + 3 d x_1 \cdot x_2^3 + 3x_1 \cdot 3 x_2^2 dx_2$, $df = -1.9$
\end{enumerate}
\end{sol}
\end{problem}


\begin{problem}
Пусть $t$ — скалярная переменная, $r$, $s$ — векторные переменные, $R$, $S$ — матричные переменные. Кроме того, $a$, $b$ — векторы констант, $A$, $B$ — матрицы констант.

Применив базовые правила дифференцирования найдите:
\begin{enumerate}
\item $d(ARB)$;
\item $d(r'r)$;
\item $d(r'Ar)$;
\item $d(R^{-1})$, воспользовавшись тем, что $R^{-1} \cdot R = I$;
\item $d \cos(r'r)$;
\item $d(r'Ar/r'r)$.
\end{enumerate}


\begin{sol}
\begin{enumerate}
\item $A(dR)B$
\item $2r'dr$
\item $r'(A'+A)dr$
\item $R^{-1}\cdot dR \cdot R^{-1}$
\item $-\sin(r'r)\cdot 2r'dr$
\item $\frac{r'(A'+A)dr \cdot r'r - r'Ar2r'dr}{(r'r)^2}$
\end{enumerate}
\end{sol}
\end{problem}


\begin{problem}
В методе наименьших квадратов минимизируется функция
\[
Q(\hb) = (y - X\hb)'(y - X\hb).
\]

\begin{enumerate}
\item Найдите $dQ(\hb)$ и $d^2Q(\hb)$;
\item Выпишите условия первого порядка для задачи МНК;
\item Выразите $\hb$ предполагая, что $X'X$ обратима.
\end{enumerate}


\begin{sol}
\begin{enumerate}
\item $dQ(\hb) = 2(y-X\hb)^T (-X) d\hb$, $d^2Q(\hb) = 2d\hb X^T X d\hb$
\item $dQ(\hb) = 0$
\item $\hb = (X^T X)^{-1} X^T y$
\end{enumerate}
\end{sol}
\end{problem}

\begin{problem}
В методе LASSO минимизируется функция
\[
Q(\hb) = (y - X\hb)'(y - X\hb) + \lambda \hb' \hb,
\]
где $\lambda$ — положительный параметр, штрафующий функцию за слишком большие значения $\hb$.

\begin{enumerate}
\item Найдите $dQ(\hb)$ и $d^2Q(\hb)$;
\item Выпишите условия первого порядка для задачи LASSO;
\item Выразите $\hb$.
\end{enumerate}

\begin{sol}
\begin{enumerate}
\item $dQ(\hb) = -2((y-X\hb)^T X + \lambda \hb^T) d\hb$, $d^2Q(\hb)=2d\hb^T(X^T X - \lambda I) d\hb$
\item $dQ(\hb) = 0$
\item $\hb = (X^T X - \lambda I)^{-1} X^T y$
\end{enumerate}
\end{sol}
\end{problem}



\begin{problem}

Пусть $A$ и $B$ — матрицы одного размера.
\begin{enumerate}
\item Докажите, что сумму $\sum_{ij} A_{ij} B_{ij}$ можно представить в виде $\tr(A'B)$.
\item Докажите, что $\tr(A'B) = \tr(AB') = \tr(B'A) = \tr(BA')$.
\end{enumerate}

\begin{sol}
\begin{enumerate}
\item $\sum_{ij} A_{ij} B_{ij} = \sum_j (\sum_i A_{ij} B_{ij}) = \sum_i (A' B)_{ii} = \tr(A'B)$

Пояснение: зафиксируем номер столбца $j$, тогда $A_{ij}$ — элемент исходной матрицы $A$, стоящий на пересечении $i$-ой строки и $j$-ого столбца.
Аналогично, $B_{ij}$ — элемент матрицы $B$, стоящий на пересечении $i$-ой строки и $j$-ого столбца.
Тогда $\sum_i A_{ij} B_{ij}$ — это скалярное произведение $j$-ого столбца матрицы $A$ на $j$-ый столбец матрицы $B$.
Заметим, что этот элемент будет стоять на диагонали матрицы $A'B$. Далее, берём следующие столбцы, находим скалярное произведение и прибавляем его к уже полученному, и так далее.
В итоге получаем сумму диагональных элементов матрицы $A'B$, что и требовалось доказать.
\item Докажем, что $\tr(A'B) = \tr(BA')$:
\[
\tr(A'B) = \sum_i (A' B)_{ii} = \sum_i \sum_j A_{ij} B_{ij} = \sum_j \sum_i B_{ij} A_{ij} = \sum_j (BA')_{jj} = \tr(BA')
\]
Заметим, что при транспонировании матрицы, её главная диагональ не меняется, значит, и сумма элементов остаётся прежней, то есть $\tr(A'B) = \tr(AB')$.
\end{enumerate}
\end{sol}
\end{problem}


\begin{problem}

Выведите формулу для $d\det X$.

\begin{sol}
Обозначим за $\widetilde{X}$ матрицу алгебраических дополнений матрицы $X$, тогда $\det X = \sum_j X_{ij} \widetilde{X}_{ij}$ для любого фиксированного $i$.
Вспомним, что $X^{-1} = (\det X)^{-1} \widetilde{X}^{T}$.
\[
\frac{\partial \det X}{\partial X_{ij}} = \widetilde{X}_{ij} \Rightarrow d \det X = \sum_{ij} \widetilde{X}_{ij} d X_{ij} = \det X  \sum_{ij} (\det X)^{-1} \widetilde{X}_{ij} d X_{ij}=\det X \tr(X^{-1} dX)
\]
\end{sol}
\end{problem}


\begin{problem}

Пусть $x_i$ — вектор-столбец $k\times 1$, $y_i$ — скаляр, равный $+1$ или $-1$, $\hb$ — вектор-столбец размера $k\times 1$. Рассмотрим функцию
\[
Q(\hb) = \sum_{i=1}^n \ln (1 + \exp(-y_ix_i'\hb)) + \lambda \hb' \hb
\]

\begin{enumerate}
\item Найдите $dQ$;
\item Найдите вектор-столбец $\grad Q$.
\end{enumerate}

\begin{sol}
\begin{enumerate}
\item $dQ = \sum_{i=1}^n  \frac{1}{1 + \exp(-y_ix_i'\hb)} \cdot \exp(-y_ix_i'\hb) \cdot (-y_i x_i') d\hb + 2\lambda \hb' d\hb$
\item $\grad Q =  \sum_{i=1}^n  \frac{ \exp(-y_ix_i'\hb) }{1 + \exp(-y_ix_i'\hb)} \cdot (-y_i x_i) + 2\lambda \hb$
\end{enumerate}
\end{sol}
\end{problem}



\section{Линейная регрессия}

\begin{problem}
Рассмотрим задачу линейной регресии
\[
Q(w) = (y - Xw)^T(y - Xw) \to \min_{w}.
\]

\begin{enumerate}
\item Найдите $dQ(w)$ и $d^2Q(w)$.
\item Выведите формулу для оптимального $w$.
\item Выведите формулу для матрицы-шляпницы (hat-matrix), связывающей вектор фактических $y$ и вектор прогнозов $\hat y = H\cdot y$.
\end{enumerate}

\begin{sol}
\begin{enumerate}
\item $dQ(w) = 2(Xw - y)^T X dw$, $d^2Q(w) = 2 dw^T X^T X dw$
\item $w = (X^T X)^{-1} X^T y$
\item $\hat y = Xw = X (X^T X)^{-1} X^T y \Rightarrow H = X (X^T X)^{-1} X^T$
\end{enumerate}
\end{sol}
\end{problem}

\begin{problem}
Рассмотрим задачу регрессии с одним признаком и без константы, $\hy_i = w \cdot x_i$. Решите в явном виде задачи МНК со штрафом:

\begin{enumerate}
\item $Q(w) = (y - \hy)^T (y - \hy) + \lambda w^2$;
\item $Q(w) = (y - \hy)^T (y - \hy) + \lambda |w|$;
\end{enumerate}

\begin{sol}
\begin{enumerate}
\item Выпишем $dQ(W)$ и найдём градиент:
\[ dQ(w) = 2(Xw - y)^T X dw + 2 \lambda w^T dw \Rightarrow \nabla Q (w) = 2 X^T (Xw - y) + 2 \lambda w \]
Приравняв градиент к нулю, получим:
\[w = (X^T X + \lambda I)^{-1} X^T y \]
\item Рассмотрим два случая.
\begin{itemize}
\item $w \geq 0$ : $Q(w) = (y - \hy)^T (y - \hy) + \lambda w \to \min_w$. Решив, получим оптимальное значение:
\[
w^{+} = \frac{x^T y}{x^T x} - \frac{\lambda}{2 x^T x}
\]
\item  $w < 0$ : $Q(w) = (y - \hy)^T (y - \hy) - \lambda w \to \min_w$. Решив, получим оптимальное значение:
\[
w^{-} = \frac{x^T y}{x^T x} + \frac{\lambda}{2 x^T x}
\]
\end{itemize}
Далее нужно заметить, что $Q(w)$ — это парабола, после чего рассмотреть четыре возможных случая расположения $w^{+}$ и $w^{-}$ и получить ответ:
\begin{itemize}
\item $w^{+} < 0, w^{-} < 0 \Rightarrow w^{*} = w^{-}$
\item $w^{+} > 0, w^{-} > 0 \Rightarrow w^{*} = w^{+}$
\item $w^{+} < 0, w^{-} > 0 \Rightarrow w^{*} = 0$
\item $w^{+} > 0, w^{-} < 0$ — этот случай невозможен
\end{itemize}
\end{enumerate}
\end{sol}
\end{problem}

\begin{problem}
Храбрая и торопливая исследовательница Мишель хочет решить задачу линейной регрессии по $n$ наблюдениям с вектором $y$ и матрицей признаков $X$. Сначала исследовательница Мишель так торопилась, что совсем забыла последнее наблюдение и оценила задачу с более коротким вектором $y^{-}$ и матрицей $X^{-}$, где не хватает последней строки. Затем Мишель взяла правильную матрицу $X$, но неправильный вектор $y^*$, в котором она вместо фактического последнего наблюдения вектора $y$ вписала его прогноз, полученный с помощью регрессии с $y^{-1}$ и $X^{-}$.

\begin{enumerate}
\item Как связаны $\hat y_n^{-}$ и $\hat y_n^{*}$ (прогнозы для последнего наблюдения полученные по модели без последнего наблюдения и модели с неверным последним наблюдением)?
\item Как выглядит вектор, равный разнице $y - y^*$?
\item Какие величины находятся в векторе $H\cdot (y - y^*)$? Чему равна последняя, $n$-ая, компонента этого вектора? Выразите её через $H_{nn}$ и ошибку прогноза последнего наблюдения по модели без последнего наблюдения, $y_n - \hat y_n^{-}$.
\item Как связаны между собой ошибка прогноза $n$-го наблюдения по полной модели, ошибка прогноза $n$-го наблюдения по модели без последнего наблюдения и $H_{nn}$?
\item Как быстро провести кросс-валидацию с выкидыванием одного наблюдения для задачи линейной регрессии?
\end{enumerate}

\begin{sol}
\[
y_n - \hat y_n = (1 - H_{nn}) (y_n - \hat y_n^-)
\]
\end{sol}
\end{problem}


\section{Линейные классификаторы}

\begin{problem}
Рассмотрим плоскость в $\RR^3$, задаваемую уравнением $5x_1 + 6x_2 -7x_3 + 10 = 0$ и две точки, $A = (2, 1, 4)$ и $B = (4, 0, 4)$.
\begin{enumerate}
  \item Найдите любой вектор, перпендикулярный плоскости.
  \item Правда ли, что отрезок $AB$ пересекает плоскость?
  \item Найдите длину отрезка $AB$;
  \item Не находя расстояние от точек до плоскости, определите, во сколько раз точка $A$ дальше от плоскости, чем точка $B$;
  \item Найдите расстояние от точки $A$ до плоскости.
\end{enumerate}

\begin{sol}
  \begin{enumerate}
  \item $(5, 6, -7)$
  \item Подставим точки $A$ и $B$ в уравнение плоскости:
  \[A: 5 \cdot 2 + 6 \cdot 1 - 7 \cdot 4 + 10 = -2 \]
  \[B: 5 \cdot 4 + 6 \cdot 0 - 7 \cdot 4 + 10 = 2 \]
  Точки $A$ и $B$ лежат по разные стороны плоскости, следовательно, отрезок $AB$ пересекает её.
  \item $\overrightarrow{AB} = (2, -1, 0)$, $|AB| = \sqrt{4 + 1 + 0} = \sqrt{5}$
  \item Расстояние одинаково % на семинаре условие было изменено
  \item $\rho = \frac{|5 \cdot 2 + 6 \cdot 1 - 7 \cdot 4 + 10|}{\sqrt{5^2 + 6^2 + 7^2}} = \frac{2}{\sqrt{10}}$
  \end{enumerate}
\end{sol}
\end{problem}


\begin{problem}
Рассмотрим простейший персептрон с константой, единственным входом $x_1$ и пороговой функцией активации. Подберите веса так, чтобы персептрон реализовывал логическое отрицание (в ответ на 0 выдавал 1, и наоборот).
\begin{sol}
Например, $w_1 = -2$, $w_0 = 2$, где $w_0$ — вес при константе.
\end{sol}
\end{problem}

\begin{problem}
Рассмотрим простейший персептрон с константой, двумя входами $x_1$, $x_2$ и пороговой функцией активации.

\todo[inline]{Здесь ассистенты нарисуют в tikz картинку, достойную стоять вместо Джоконды в Лувре}

\begin{enumerate}
\item Подберите веса так, чтобы персептрон реализовывал логическое ИЛИ (OR).
\item Подберите веса так, чтобы персептрон реализовывал логическое И (AND).
\item Докажите, что веса невозможно подобрать так, чтобы персептрон реализовывал исключающее логическое ИЛИ (XOR).
\item Добавьте персептрону вход $x_3 = x_1 \cdot x_2$ и подберите веса так, чтобы персептрон реализовывал XOR.
\item Реализуйте XOR с помощью трёх персептронов с двумя входами и константой. Укажите веса и схему их взаимосвязей.
\end{enumerate}

\begin{sol}
\begin{enumerate}
\item Например, $w_1 = 2, w_2 = 2, w_0 = 0$
\item Например, $w_1 = 2, w_2 = 2, w_0 = -2$
\item Можно показать графически: нарисовать на плоскости точки $(0,0), (1,1), (1,0), (0,1)$, причём для первых двух
нейрон должен выдавать ответ $0$, а для вторых — $1$. Чтобы разделить эти точки, необходимо провести две прямые,
в то время как один нейрон проводит только одну.
\item Например, подойдут веса $w_1 = 3, w_2 = 3, w_3 = -5, w_0 = -1$
\item Первый нейрон с весами $w_{11} = 1, w_{12} = 1, w_{10} = 1/2$ и второй нейрон с весами $w_{21} = 1, w_{22} = 1, w_{10} = -1/2$
должны подавать реузльтаты на вход третьему нейрону с весами $w_{31} = 3, w_{32} = -1, w_{30} = -2$
\end{enumerate}
\end{sol}
\end{problem}



\begin{problem}
В коробке завалялось три персептрона, у каждого два входа с константой и пороговая функция активации. Реализуйте с их помощью функцию
\[
y = \begin{cases}
1, \text{ если } x_2 \geq |x_1 - 3| + 2; \\
0, \text{ иначе}
\end{cases}.
\]
\begin{sol}
\end{sol}
\end{problem}



\begin{problem}
Рассмотрим следующий набор данных:

\begin{tabular}{ccc}
\toprule
$x_i$ & $z_i$ & $y_i$ \\
\midrule
-1 & -1 & 0 \\
1 & -1 & 0 \\
-1 & 1 & 0 \\
1 & 1 & 0 \\
0 & 2 & 1 \\
2 & 0 & 1 \\
0 & -2 & 1 \\
-2 & 0 & 1 \\
\bottomrule
\end{tabular}

\begin{enumerate}
\item Существует ли перспетрон с константой, двумя входами и пороговой функцией активации, способный идеально классифицировать $y_i$ на данной выборке? А хватит ли двух таких персептронов? А может хватит трёх?
\item Введите такое преобразование исходных признаков $h_i = h(x_i, z_i)$, при котором с идеальной классификацией $y_i$ справился бы даже персептрон с одним входом, константой и пороговой функцией активации.
\end{enumerate}


\begin{sol}
\end{sol}
\end{problem}





\begin{problem}
Бандерлог из Лога\footnote{деревня в Кадуйском районе Вологодской области} ведёт блог, любит считать логарифмы и оценивать логистические регрессии. С помощью нового алгоритма Бандерлог решил задачу классификации по трём наблюдениям и получил $b_i = \hat\P(y_i = 1|x_i)$.

\begin{tabular}{cc}
  \toprule
  $y_i$ & $b_i$ \\
  \midrule
  1 & 0.7 \\
  -1 & 0.2 \\
  -1 & 0.3 \\
  \bottomrule
\end{tabular}

\begin{enumerate}
\item Постройте ROC-кривую.
\item Найдите площадь под ROC-кривой и индекс Джини.
\item Постройте PR-кривую (кривая точность-полнота).
\item Найдите площадь под PR-кривой.
\item Как по-английски будет «бревно»?
\end{enumerate}
\begin{sol}
\end{sol}
\end{problem}



\begin{problem}
Классификатор Бандерлога имеет вид
\[
a_i = \begin{cases}
1, \text{ если } b_i > t; \\
-1, \text{ иначе.}
\end{cases}
\]

Докажите, что площадь под ROC-кривой равна вероятности того, случайно выбранный положительный объект окажется позже случайно выбранного отрицательного объекта, если объекты ранжированы по возрастанию величины $b_i$.
\begin{sol}
\end{sol}
\end{problem}






\begin{problem}
Все средние издалека выглядят одинаково, $\text{среднее}=f^{-1}(0.5f(x_1) + 0.5f(x_2))$.  Например, у среднего арифметического $f(t)=t$, у среднего гармонического $f(t)=1/t$.

\begin{enumerate}
  \item Какая $f$ используется для среднего геометрического?
\end{enumerate}

Для измерения качества бинарной классификации Ара использует среднее арифметическое точности и полноты, Гена — среднее геометрическое, а Гарик — среднее гармоническое.

\begin{enumerate}[resume]
  \item У кого будут выходить самые «качественные» и самые «некачественные» прогнозы?
\end{enumerate}


\begin{sol}
\begin{enumerate}
\item $f(t) = \log(t)$
\item Среднее гармоническое < среднее геометрическое < среднее арифметическое
\end{enumerate}
\end{sol}
\end{problem}


\begin{problem}
Бандерлог начинает все определения со слов «это доля правильных ответов»:
\begin{enumerate}
  \item accuracy — это доля правильных ответов\ldots
  \item точность (precision) — это доля правильных ответов\ldots
  \item полнота (recall) — это доля правильных ответов\ldots
  \item TPR — это доля правильных ответов\ldots
\end{enumerate}

Закончите определения Бандерлога так, чтобы они были, хм, правильными.
\begin{sol}
\begin{enumerate}
\item $\text{accuracy} = \frac{\text{TP} + \text{TN}}{\text{TP} +\text{FP} +\text{FN} +\text{TN}}$
\item $\text{precision} = \frac{\text{TP}}{\text{TP} +\text{FP}}$
\item $\text{recall} = \frac{\text{TP}}{\text{TP} +\text{FN}}$
\item $\text{TPR} = \frac{\text{TP}}{\text{TP} +\text{FN}}$
\end{enumerate}
\end{sol}
\end{problem}


\begin{problem}
Алгоритм бинарной классификации, придуманный Бандерлогом, выдаёт оценки вероятности $b_i = \hat\P(y_i=1 | x_i)$. Всего у Бандерлога 10000 наблюдений. Если ранжировать их по возрастанию $b_i$, то окажется что наблюдения с $y_i = 1$ занимают ровно места с  5501 по 5600.

Найдите площадь по ROC-кривой и площадь под PR-кривой.
\begin{sol}
\end{sol}
\end{problem}



\begin{problem}
Бандерлог собрал выборку из 900 муравьёв и 100 китов. Переменная $y_i$ равна $1$ для китов. Бандерлог хочет, чтобы его алгоритм классификации выдавал для каждого наблюдения число $b_i=f(x_i) \in [0;1]$, оценку вероятности того, что наблюдение является китом. В качестве признака Бандерлог использует количество глаз, не задумавшись о том, что оно равно двум и для муравьёв, и для китов.

Решите задачу минимизации эмпирической функции риска и найдите все $b_i$ для функций потерь:
\begin{enumerate}
  \item $L(y_i, b_i) = (y_i - b_i)^2$, если для муравьёв $y_i = 0$;
  \item $L(y_i, b_i) = |y_i - b_i|$, если для муравьёв $y_i = 0$;
  \item $L(y_i, b_i) = \begin{cases}
  -\log b_i, \text{ если } y_i = 1 \\
  -\log (1-b_i), \text{ иначе.}
  \end{cases}$;
  %\item $L(y_i, b_i) = \log (1 + \exp(-y \cdot b))$, если для муравьёв $y = -1$;
  \item $L(y_i, b_i) = \begin{cases}
  1/b_i, \text{ если } y_i = 1 \\
  1/(1-b_i), \text{ иначе.}
  \end{cases}$;
\end{enumerate}
\begin{sol}
\begin{enumerate}
\item Поскольку все признаки одинаковы, то $\forall i \quad b_i = f(x_i) = b$, и функционал ошибки иммет вид:
\[Q(b) = \frac{1}{1000} \sum_{i=1}^{1000} L(y_i, b) = \frac{1}{1000} \left(\sum_{i=1}^{900} b^2 + \sum_{i=1}^{100} (1-b)^2\right) \to \min_{b} \]
Дифференцируем и находим $b$:
\[2 \cdot 900 \cdot b - 2 \cdot 100 \cdot (1-b) = 0 \Rightarrow b = 0.1 \]
\item Аналогично:
\[Q(b) =  \frac{1}{1000} \sum_{i=1}^{1000} |y_i - b| = \frac{1}{1000} \left(900 \cdot b + 100|1-b| \right) \to \min_{b} \]
При $b=1$, получаем: $Q(1) = 0.9$
При $b<1$: $Q(b) = \frac{1}{1000} \left(900b + 100 - 100b \right) = \frac{1}{1000} \left(900b+100 \right)\to \min_{b}$.
Минимум функционала ошибки достигается при $b=0$ и равен $0.1$.
\item Снова выпишем функционал ошибки:
\[Q(b) = \frac{1}{1000} \left(-900 \log(1-b) + 100 \log b  \right) \to \min_b \]
Берём производную и получаем оптимальный $b$:
\[\frac{1}{1000} \left(\frac{900}{1-b} - \frac{100}{b} \right) = 0 \Rightarrow b = 0.1 \]
\item \[Q(b) = \frac{1}{1000} \left(\frac{900}{1-b} - \frac{100}{b} \right) \to \min_b  \]
Дифференцируя, получим:
\[\frac{900}{(1-b)^2} =  \frac{100}{b^2} \Rightarrow b = 0.25 \]
\end{enumerate}
\end{sol}
\end{problem}


\begin{problem}
Бандерлог утверждает, что открыл новую верхнюю границу для пороговой функции потерь, $\tilde{L}(M_i) = 1 + \frac{1}{\pi} \cdot \arctan(-x_i)$, где $M_i = y_i \cdot \langle w, x_i \rangle$. Прав ли бандерлог?
% уточнить, какая именно пороговая функция потерь, где-то порог по 0, а где-то по 1
\begin{sol}
  Нет. Не выполнено $\tilde{L} \geq L$ для всех $M \in \RR$.
\end{sol}
\end{problem}


\begin{problem}
Бандерлог из Лога оценил логистическую регрессию по четырём наблюдениям и одному признаку с константой, получил $b_i = \hat\P(y_i = 1|x_i)$, но потерял последнее наблюдение:

\begin{tabular}{cc}
  \toprule
  $y_i$ & $b_i$ \\
  \midrule
  1 & 0.7 \\
  -1 & 0.2 \\
  -1 & 0.3 \\
  ? &  ? \\
  \bottomrule
\end{tabular}

\begin{enumerate}
\item Выпишите функцию потерь для задачи логистической регрессии.
\item Выпишите условие первого порядка по коэффициенту перед константой.
\item Помогите бандерлогу восстановить пропущенные значения!
\end{enumerate}

\begin{sol}
$\hat\P(y_i = 1|x_i) = \frac{1}{1+\exp(-\beta_1 - \beta_2 x_i)}$
\begin{enumerate}
\item $loss(\beta_1, \beta_2) = - \sum_{i=1}^l \left([y_i = 1] \ln  \frac{1}{1+\exp(-\beta_1 - \beta_2 x_i)} + [y_i = -1] \ln \left(1 - \frac{1}{1+\exp(-\beta_1 - \beta_2 x_i)}\right)\right)$
\item $\frac{\partial loss}{\partial \beta_1} = - \sum_{i=1}^l \left([y_i = 1] \cdot \frac{1}{1 + \exp(\beta_1 + \beta_2 x_i)} + [y_i = -1] \cdot (-1) \cdot \frac{1}{1 + \exp(-\beta_1 - \beta_2 x_i)} \right)$
\item $y_4 = 1$, $x_4 = 0.8$
\end{enumerate}
\end{sol}
\end{problem}


\begin{problem}
У Бандерлога три наблюдения, первое наблюдение — кит, остальные — муравьи. Киты кодируются $y_i = 1$, муравьи — $y_i = -1$. На этот раз Бандерлог, чтобы быть уверенным, что $x_i$ различаются, сам лично определил $x_i = i$. После этого Бандерлог оценивает логистическую регрессию с константой.

\begin{enumerate}
  \item Выпишите эмпирическую функцию риска, которую минимизирует Бандерлог;
  \item При каких оценках коэффициентов логистической регрессии эта функция достигает своего минимума?
\end{enumerate}

\begin{sol}
\end{sol}
\end{problem}

\begin{problem}
Рассмотрим целевую функцию логистической регрессии с константой
\[
Q(w) = \frac{1}{\ell} \sum L(y_i, b_i),
\]
где $b_i = 1 / (1 + \exp( -\langle w, x_i\rangle)$ и $L(y_i, b_i) = \begin{cases}
-\log b_i, \text{ если } y_i = 1 \\
-\log (1-b_i), \text{ иначе.}
\end{cases}$.

\begin{enumerate}
\item Найдите $dQ(w)$ и $d^2Q(w)$;
\item Найдите $dQ(0)$ и $d^2Q(0)$;
\item Выпишите квадратичную аппроксимацию для $Q(w)$ в окрестности $w=0$;
\item С какой задачей совпадает задача минимизации квадратичной аппроксимации?
\end{enumerate}

\begin{sol}
\end{sol}
\end{problem}



\begin{problem}
Винни-Пух знает, что мёд бывает правильный, $honey_i=1$, и неправильный, $honey_i=0$. Пчёлы также бывают правильные, $bee_i=1$, и неправильные, $bee_i=0$. По 100 своим попыткам добыть мёд Винни-Пух составил таблицу сопряженности:

\begin{tabular}{c|cc}
\toprule
 & $honey_i=1$ & $honey_i=0$ \\
\midrule
$bee_i=1$ & 12 & 36 \\
$bee_i=0$ & 32 & 20 \\
\bottomrule
\end{tabular}

Винни-Пух использует логистическую регрессию с константой для прогнозирования правильности мёда с помощью правильности пчёл.

\begin{enumerate}
\item Какие оценки коэффициентов получит Винни-Пух?
\item Какой прогноз вероятности правильности мёда при встрече с неправильными пчёлами даёт логистическая модель? Как это число можно посчитать без рассчитывания коэффициентов?
\end{enumerate}

\begin{sol}
\begin{enumerate}
\item Выпишем аппроксимацию функции потерь:
\[
loss(\beta_1, \beta_2) \approx 100 \ln 2 + 6 \beta_1 + 12 \beta_2 + \frac{1}{2}(25 \beta_1^2 + 2 \cdot 12 \beta_1 \beta_2 + 12 \beta_2^2) \to \min_{\beta_1, \beta_2}
\]
Взяв производные по $\beta_1$ и $\beta_2$, получим $\hb_1 = \frac{6}{13}$, $\hb_2 = - \frac{19}{13}$.
\item $\hat{P}(honey_i = 1 | bee_i = 0) = \frac{1}{1+\exp(-6/13)} \approx 0.615$.

Это же число можно было получить из таблицы: $\frac{32}{32 + 20} \approx 0.61$.
\end{enumerate}
\end{sol}
\end{problem}


\begin{problem}
Винни-Пух оценил логистическую регрессию для прогнозирования правильности мёда от высоты дерева (м) $x_i$ и удалённости от дома (км) $z_i$: $\ln odds_i = 2+0.3x_i - 0.5z_i$.
\begin{enumerate}
\item Оцените вероятность того, что $y_i=1$ для $x=15$, $z=3.5$.
\item Оцените предельный эффект увеличения $x$ на единицу на вероятность того, что $y_i=1$ для $x=15$, $z=3.5$.
\item При каком значении $x$ предельный эффект увеличения $x$ на единицу в точке $z=3.5$ будет максимальным?
\end{enumerate}

\begin{sol}
Предельный эффект максимален при максимальной производной $\Lambda'(\hat \beta_1 + \hat\beta_2x + \hat\beta_3z)$, то есть при $\hat \beta_1 + \hat\beta_2x + \hat\beta_3z=0$.
\end{sol}
\end{problem}

% \section{Хочу ещё задач!}



\section{Матрицы}


\begin{problem}
Известна матрица $X$,
\[
X = \begin{pmatrix}
1 & 1 \\
0 & 1 \\
-1 & 0 \\
\end{pmatrix};
\]

\begin{enumerate}
\item Найдите QR-разложение матрицы $X'X$;
\item Найдите QR-разложение матрицы $XX'$;
\item Найдите спектральное разложение матрицы $X'X$;
\item Найдите спектральное разложение матрицы $XX'$;
\item Найдите сингулярное разложение (SVD) матрицы $X$;
\end{enumerate}

\begin{sol}
\begin{enumerate}
\item $X'X =
\begin{pmatrix}
2/\sqrt{5} & -1/\sqrt{5} \\
1/\sqrt{5} & 2/\sqrt{5} \\
\end{pmatrix}
\begin{pmatrix}
\sqrt{5} & 4/\sqrt{5} \\
0 & 3/\sqrt{5} \\
\end{pmatrix}$
\item $XX' =
\begin{pmatrix}
2/\sqrt{6} & 0 & 1/\sqrt{6} \\
1/\sqrt{6} & 1/\sqrt{2} & 1/\sqrt{6} \\
-1/\sqrt{6} & 1/\sqrt{2} & 1/\sqrt{6} \\
\end{pmatrix}
\begin{pmatrix}
\sqrt{6} & 3/\sqrt{6} & -3/\sqrt{6} \\
0 & 1/\sqrt{2} & 1/\sqrt{2} \\
0 & 0 & 0 \\
\end{pmatrix}
$
\item $X'X =
\begin{pmatrix}
1/\sqrt{2} & -1/\sqrt{2} \\
1/\sqrt{2} & 1/\sqrt{2} \\
\end{pmatrix}
\begin{pmatrix}
3 & 0 \\
0 & 1 \\
\end{pmatrix}
\begin{pmatrix}
1/\sqrt{2} & 1/\sqrt{2} \\
1s/\sqrt{2} & -1/\sqrt{2} \\
\end{pmatrix}
$
\item $XX' =
\begin{pmatrix}
2/\sqrt{6} & 0 & 1/\sqrt{3} \\
1/\sqrt{6} & 1/\sqrt{2} & -1/\sqrt{3} \\
-1/\sqrt{6} & 1/\sqrt{2} & 1/\sqrt{3} \\
\end{pmatrix}
\begin{pmatrix}
3 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 0 \\
\end{pmatrix}
\begin{pmatrix}
2/\sqrt{6} & 1/\sqrt{6} & -1/\sqrt{6} \\
0 & 1/\sqrt{2} & 1/\sqrt{2} \\
1/\sqrt{3} & -1/\sqrt{3} & 1/\sqrt{3} \\
\end{pmatrix}
$
\item $X =
\begin{pmatrix}
2/\sqrt{6} & 0 & 1/\sqrt{3} \\
1/\sqrt{6} & 1/\sqrt{2} & -1/\sqrt{3} \\
-1/\sqrt{6} & 1/\sqrt{2} & 1/\sqrt{3} \\
\end{pmatrix}
\begin{pmatrix}
\sqrt{3} & 0 \\
0 & 1 \\
0 & 0 \\
\end{pmatrix}
\begin{pmatrix}
1/\sqrt{2} & 1/\sqrt{2} \\
-1/\sqrt{2} & 1/\sqrt{2} \\
\end{pmatrix}
$
\end{enumerate}
\end{sol}
\end{problem}


\begin{problem}
Объясните геометрический смысл QR, SVD и спектрального разложений.
\begin{sol}
\end{sol}
\end{problem}



\begin{problem}
Бандрелог выполнил SVD-разложение матрицы регрессоров $X$. Помогите Бандерлогу поскорее найти формулу для матрицы-шляпницы $H$, которая проецирует $y$ на пространство столбцов матрицы $X$, $\hat y = Hy$.
\begin{sol}
$H = UU'$
\end{sol}
\end{problem}




\begin{problem}
Бандрелог выполнил QR-разложение матрицы регрессоров $X$. Помогите Бандерлогу поскорее найти формулу для матрицы-шляпницы $H$, которая проецирует $y$ на пространство столбцов матрицы $X$, $\hat y = Hy$.
\begin{sol}
$H = QQ'$
\end{sol}
\end{problem}

\section{Метод опорных векторов}



\begin{problem}
На плоскости имеются точки двух цветов. Красные: $(1,1)$, $(1,-1)$ и синие: $(-1,1)$, $(-1,-1)$.
\begin{enumerate}
\item Найдите разделяющую гиперплоскость методом опорных векторов при разных $C$.
\item Укажите опорные вектора.
\end{enumerate}



\begin{sol}
\end{sol}
\end{problem}


\begin{problem}
На плоскости имеются точки двух цветов. Красные: $(1,1)$, $(1,-1)$ и синие: $(-1,1)$, $(-1,-1)$ и $(2,0)$.
\begin{enumerate}
\item Найдите разделяющую гиперплоскость методом опорных векторов при разных $C$.
\item Укажите опорные вектора.
\end{enumerate}



\begin{sol}
\end{sol}
\end{problem}




\begin{problem}
Эконометресса Авдотья решила использовать метод опорных векторов с гауссовским ядром с параметром $\sigma=1$ и штрафным коэффициентом $C=1$. Соответственно, она минимизировала целевую функцию

\[
\frac{w'w}{2} + C\sum_{i=1}^n \xi_i,
\]

где разделяющая плоскость задаётся $w'x-w_0=0$, а $\xi_i$ — размеры «заступа» за разделяющую полосу.

Затем Автдотья подумала, что неплохо бы выбрать наилучшие $C$ и $\sigma$. Ей лень было использовать кросс-валидацию, поэтому Авдотья минимизировала данную функцию по $C\geq 0$ и $\sigma\geq 0$. Какие значения она получила?


\begin{sol}
$C=0$ и $\sigma=+\infty$
\end{sol}
\end{problem}


\begin{problem}
Задан вектор $w=(2,3)$ и число $w_0=7$.

\begin{enumerate}
\item Нарисуйте прямые $\langle w, x\rangle=w_0$, $\langle w, x\rangle=w_0+1$, $\langle w, x\rangle=w_0-1$.
\item Найдите ширину полосы между $\langle w, x\rangle=w_0+1$ и $\langle w, x\rangle=w_0-1$.
\item Найдите расстояние от точки $(5,6)$ до прямой $\langle w, x\rangle=w_0-1$.
\end{enumerate}

\begin{sol}
\begin{enumerate}
\item Нужно нарисовать прямые $2x_1 + 3 x_2 = 7$, $2x_1 + 3 x_2 = 8$, $2x_1 + 3 x_2 = 6$.
\item $2/\sqrt{13}$
\item $22/\sqrt{13}$
\end{enumerate}
\end{sol}
\end{problem}


\begin{problem}
Заданы две прямые, $l_0$: $x^{(1)}+3x^{(2)} = 9$ и $l_1$: $x^{(1)}+3x^{(2)} = 13$. Найдите подходяющий вектор $w$ и число $w_0$ так, чтобы прямая $l_0$ записывалась как  $\langle w,x \rangle=w_0-1$, а прямая $l_1$ как  $\langle w,x\rangle=w_0 + 1$.
\begin{sol}
$w = (1/2, 1/2)$, $w_0 = 5.5$
\end{sol}
\end{problem}


\begin{problem}
Даны наблюдения

\begin{tabular}{ccc}
 $x^{(1)}$ & $x^{(2)}$ & $y$ \\
\midrule
 1 & 0 & 0 \\
 2 & 0 & 0 \\
 0 & 3 & 1 \\
 0 & 4 & 1 \\
\end{tabular}

\begin{enumerate}
\item Нарисуйте разделяющую полосу наибольшей ширины.
\item Решите задачу оптимизации
\[
\min_{w, w_0} \frac{1}{2} \langle w, w \rangle
\]

при ограничении: для $y_i=1$ выполнено условие $\langle w,x \rangle \geq w_0 + 1$, а для $y_i=0$ выполнено условие $\langle w,x \rangle \leq w_0 - 1$.
\item Для точки $x=(x^{(1)}, x^{(2)}) =(1, 1)$ найдите значение $\langle w,x \rangle-w_0$ и постройте прогноз $\hy$.
\end{enumerate}

\begin{sol}
\end{sol}
\end{problem}

\begin{problem}

По картинке качественно решите задачу разделения точек:



\begin{minipage}{0.6\textwidth}
\begin{center}
%\begin{tikzpicture}[scale = 0.025]
\includegraphics[scale=0.2]{images/armada.png}
%\input{armada.tikz}
%\end{tikzpicture}
\end{center}
\end{minipage}


Целевая функция имеет вид:
\[
\min_{w, w_0} \frac{1}{2} w'w + C \sum_{i=1}^n \xi_i
\]

Уравнение разделяющей поверхности — $w'x = w_0$, уравнения краёв полосы: $w'x=w_0+1$ и $w'x=w_0-1$. Нарушителями считаются наблюдения, которые попали на нейтральную полосу или на чужую территорию. Здесь $\xi_i = |w| \cdot d_i$, где $d_i$ — длина «заступ» наблюдения за черту «своих».



\begin{enumerate}
\item Как пройдёт разделяющая полоса при $C=1$? Найдите $w$, $w_0$, и величины штрафов $\xi_i$.
\item Как пройдёт разделяющая полоса при $C=+\infty$? Найдите $w$, $w_0$, и величины штрафов $\xi_i$.
\end{enumerate}
\begin{sol}
\end{sol}
\end{problem}




\begin{problem}
ююю
\begin{sol}
\end{sol}
\end{problem}


\newpage
\section{Ядра к бою!}


\begin{problem}
Ядерная функция, скалярное произведение в расширяющем пространстве, имеет вид $K(a,b)=\exp(-|a-b|^2)$.

Имеются вектора $a=(1,1,1)$ и $b=(1,2,0)$.

Найдите длину векторов и косинус угла между ними в исходном и расширяющем пространстве.


\begin{sol}
В исходном пространстве: $|\vec{a}|=\sqrt{3}$, $|\vec{b}|=\sqrt{5}$, $\cos(\vec{a},\vec{b})=\sqrt{0.6}$.

В расширяющем пространстве: $|h(\vec{a})|=1$, $|h(\vec{b})|=1$, $\cos(h(\vec{a}),h(\vec{b}))=e^{-2}$.
\end{sol}
\end{problem}


\begin{problem}
Рассмотрим два вектора, $v_1=(1, 1, 2)$ и $v_2=(1, 1, 1)$. Переход в спрямляющее пространство осуществляется с помощью гауссовской ядерной функции с параметром $\gamma$, $k(v,v')=\exp(-\gamma |v-v'|^2)$.

\begin{enumerate}
\item  Как от $\gamma$ зависят длины векторов в спрямляющем пространстве?
\item  Как от $\gamma$ зависит угол между векторами в спрямляющем пространстве?
\end{enumerate}



\begin{sol}
Длина равна 1 и не зависит от $\gamma$. При $\gamma \approx 0$ вектора примерно совпадают, при больших $\gamma$ вектора примерно ортогональны.
\end{sol}
\end{problem}





\begin{problem}
Имеются три наблюдения $A$, $B$ и $C$:

\begin{tabular}{ccc}
 & $x$ & $y$ \\
\midrule
$A$ & 1 & -2 \\
$B$ & 2 & 1 \\
$C$ & 3 & 0 \\
\end{tabular}

\begin{enumerate}
\item Найдите расстояние $AB$ и косинус угла $ABC$.
\item Найдите расстояние $AB$ и косинус угла $ABC$ в расширенном пространстве с помощью гауссовского ядра с $K(x,x') =\exp(-|x-x'|^2)$.
\item Найдите расстояние $AB$ и косинус угла $ABC$ в расширенном пространстве с помощью полиномиального ядра второй степени.
\end{enumerate}

\begin{sol}
\begin{enumerate}
\item $|AB| = \sqrt{10}$, $\cos(ABC) = \frac{1}{\sqrt{5}}$
\item $|AB| = 1$, $\cos(ABC) = e^{-8}$
\end{enumerate}
\end{sol}
\end{problem}


\begin{problem}
Переход из двумерного пространства в расширяющее задан функцией
\[
f : (x_1,x_2) \to (1,x_1,x_2,3x_1 x_2, 2x_1^2, 4x_2^2).
\]
Найдите соответствующую ядерную функцию.

\begin{sol}
$K(x, y) = 1 + x_1 y_1 + x_2 y_2 + 3x_1 x_2 \cdot 3 y_1 y_2 + 2 x_1^2 \cdot 2 y_1^2 + 4 x_2^2 \cdot 4 y_2^2$
\end{sol}
\end{problem}



\begin{problem}
Ядерная функция имеет вид
\[
K(x,y)=x_1^2y_1^2+x_2^2y_2^2+2x_1x_2y_1y_2.
\]
Как может выглядеть функция $f:\R^2\to\R^3$ переводящие исходные векторы в расширенное пространство?


\begin{sol}
$f(x_1,x_2)=(x_1^2,x_2^2,\sqrt{2}x_1x_2)$
\end{sol}
\end{problem}

\newpage
\begin{problem}
  Является ли функция $K(x,z)$ ядром?
  \begin{enumerate}
    \item $K(x, z) = \begin{cases}
      1, \text{ if } x = z; \\
      0, \text{ otherwise } \\
    \end{cases}$;
    \item $K(x, z) = \begin{cases}
      0, \text{ if } x = z; \\
      1, \text{ otherwise } \\
    \end{cases}$;
    \item $K(x, z) = \sin( x^T z )$;
    \item $K(x, z) = \cos(x^Tx)\sin(z^Tz)$;
  \end{enumerate}

\begin{sol}
Ядром является только функция в пункте 1.
\end{sol}
\end{problem}


\begin{problem}
Пусть $x$ и $z$ — строки символов, возможно разной длины. Рассмотрим две функции. Функция $K_1(x, z)$ равна единице, если строки $x$ и $z$ совпадают. Функция $K_2(x, z)$ — число совпадающих подстрок. Функция $K_3$ — произведение количеств букв «а» в обеих словах.
\begin{enumerate}
  \item Найдите $K_1(\text{«мама»},\text{«ам»})$ и $K_2(\text{«мама»},\text{«ам»})$, $K_3(\text{«мама»},\text{«ам»})$
  \item Является ли функция $K_1$ ядром?
  \item Является ли функция $K_2$ ядром?
  \item Является ли функция $K_3$ ядром?
\end{enumerate}

\begin{sol}
\end{sol}
\end{problem}




\begin{problem}
На прямой аллее растёт три дуба. Находятся в точках с координатами $x_1 = 1$, $x_2 = 2$ и $x_3 = -3$. Исследователь Винни-Пух проверил и выяснил, что на втором Дубе водятся правильные пчёлы, а на остальных — неправильные.

\begin{enumerate}
\item Являются ли пчёлы линейно разделимыми в пространстве исходной аллеи?
\item Помогите Винни-Пуху выписать прямую задачу метода опорных векторов в пространстве исходной аллеи;
\item Помогите Винни-Пуху выписать двойственную задачу метода опорных векторов в пространстве исходной аллеи;
\item Помогите Винни-Пуху выписать двойственную задачу метода опорных векторов в бесконечномерном пространстве с ядерной функцией $K(x, z) = \exp(-(x-z)^2)$; Являются ли точки в нём линейно разделимыми?

\item Помогите Винни-Пуху выписать прямую и двойственную задачу метода опорных векторов в спрямляющем пространстве с ядерной функцией $K(x, z) = (xz + 1)^2$; Являются ли точки в нём линейно разделимыми?
\end{enumerate}

\begin{sol}
\end{sol}
\end{problem}





\section{Двойственные задачи}


\begin{problem}
Выпишите двойственную задачу для минимизации $x_1^2 + x_2^2 + x_3^2$ при ограничении $2x_1 + 3x_2 +5x_3 = 10$.
\begin{sol}
Выпишем лагранжиан:
\[
L(x_1, x_2, x_3, \lambda) = x_1^2 + x_2^2 + x_3^2 + \lambda(2x_1 + 3x_2 +5x_3 - 10)
\]
Затем условие первого порядка:
\[
\begin{cases}
\frac{\partial L}{\partial x_1} = 2x_1 + 2 \lambda = 0 \Rightarrow & x_1 = -\lambda \\
\frac{\partial L}{\partial x_2} = 2x_2 + 3 \lambda = 0 \Rightarrow & x_2 = -\frac{3}{2}\lambda \\
\frac{\partial L}{\partial x_3} = 2x_3 + 5 \lambda = 0 \Rightarrow & x_3 = -\frac{5}{2}\lambda \\
\end{cases}
\]
Двойственная задача имеет вид:
\[
g(\lambda) = (-\lambda)^2 + \left(-\frac{3}{2}\lambda\right)^2 + \left(-\frac{5}{2}\lambda\right)^2 + \lambda\left(-2\lambda + 3 \cdot \left(-\frac{3}{2}\right)\lambda + 5 \cdot \left(-\frac{5}{2}\right)\lambda -10 \right) \to \max_{\lambda}
\]
\end{sol}
\end{problem}


\begin{problem}
Выпишите двойственную задачу для $x_1 + 2x_2 + 3x_3 \to \max$ при ограничениях $x_1 + x_2 + x_3 \leq 10$, $2x_1+x_2+x_3 \leq 10$, все $x_i \geq 0$.
\begin{sol}
\end{sol}
\end{problem}




\begin{problem}
Выпишите двойственную задачу для максимизации $1/x_1 + 2/x_2$ при ограничении $2x_1 + 3x_2 = 10$ и $x_1 \in [1;10]$, $x_2 \in [2;6]$.
\begin{sol}
\end{sol}
\end{problem}

\begin{problem}
Выпишите двойственную задачу для минимизации $f(x) = \frac{1}{2} x'Hx + g'x$ при ограничении $A'x=b$.
\begin{sol}
\end{sol}
\end{problem}


\begin{problem}
Выпишите двойственную задачу для минимизации $f(x) = \frac{1}{2} x'Hx + g'x$ при ограничении $A'x \leq b$.
\begin{sol}
\end{sol}
\end{problem}

\begin{problem}
Выпишите прямую и двойственную задачу для метода опорных векторов в исходном пространстве.
\begin{sol}
\end{sol}
\end{problem}

\begin{problem}
Выпишите прямую и двойственную задачу для метода опорных векторов в спрямляющем пространстве с использованием ядра $K(.,.)$.
\begin{sol}
\end{sol}
\end{problem}





\section{Метод главных компонент}



\begin{problem}
Найдите прямую, у которой сумма квадратов расстояний до точек $(0,0)$, $(1, 1)$, $(2, 1)$ будет минимальной. Чему равна при этом доля объяснённого разброса точек?
\begin{sol}
\end{sol}
\end{problem}

\begin{problem}
Есть две переменных, $x = (1, 0, 0, 3)'$, $z = (3, 2, 0, 3)'$. Найдите первую и вторую главные компоненты.
\begin{sol}
Матрица с центрированными столбцами имеет вид: $\widetilde{X} = \begin{pmatrix}
0 & 1 \\
-1 & 0 \\
-1 & -2 \\
2 & 1 \\
\end{pmatrix}$

Тогда $\widetilde{X}'\widetilde{X} =
\begin{pmatrix}
6 & 4 \\
4 & 6 \\
\end{pmatrix}$.

Её собственные числа: $\lambda_1 = 10$, $\lambda_2 = 2$, собственные вектора $v_1 = (1/\sqrt{2} \quad 1/\sqrt{2})'$, $v_2 =  (1/\sqrt{2} \quad -1/\sqrt{2})$.
Найдём главные компоненты:
\[
P = XV =
\begin{pmatrix}
0 & 1 \\
-1 & 0 \\
-1 & -2 \\
2 & 1 \\
\end{pmatrix}
\begin{pmatrix}
1/\sqrt{2} & 1/\sqrt{2} \\
1/\sqrt{2} & -1/\sqrt{2} \\
\end{pmatrix} =
\begin{pmatrix}
1/\sqrt{2} & -1/\sqrt{2} \\
-1/\sqrt{2} & -1/\sqrt{2} \\
-3/\sqrt{2} & 1/\sqrt{2} \\
3/\sqrt{2} & 1/\sqrt{2}\\
\end{pmatrix}
\]
Первая и вторая главные компоненты — это первый и второй столбцы матрицы $P$ соответственно.
\end{sol}
\end{problem}


\begin{problem} % у этой матрицы нельзя руками найти собственные значения
Известна матрица выборочных ковариаций трёх переменных. Для удобства будем считать, что переменные уже центрированы.

\[
\begin{pmatrix}
4 & 1 & -1 \\
1 & 5 & 0 \\
-1 & 0 & 9
\end{pmatrix}
\]

\begin{enumerate}
\item Выразите первую и вторую главные компоненты через три исходных переменных.
\item Выразите первую и вторую главные компоненты, через три исходных переменных, если перед методом главных компонент переменные необходимо стандартизировать.
\end{enumerate}
\begin{sol}
\end{sol}
\end{problem}




\begin{problem}
Пионеры, Крокодил Гена и Чебурашка собирали металлолом несколько дней подряд. В распоряжение иностранной шпионки, гражданки Шапокляк, попали ежедневные данные по количеству собранного металлолома: вектор $g$ — для Крокодила Гены, вектор $h$ — для Чебурашки и вектор $x$ — для Пионеров. Гена и Чебурашка собирали вместе, поэтому выборочная корреляция $\sCorr(g,h)=-0.9$. Гена и Чебурашка собирали независимо от Пионеров, поэтому выборочные корреляции $\sCorr(g,x)=0$, $\sCorr(h,x)=0$. Если регрессоры $g$, $h$ и $x$ центрировать и нормировать, то получится матрица $\tilde{X}$.
\begin{enumerate}
\item Найдите параметр обусловленности матрицы $(\tilde{X}'\tilde{X})$.
\item Вычислите одну или две главные компоненты (выразите их через вектор-столбцы матрицы. $\tilde{X}$), объясняющие не менее 70\% общей выборочной дисперсии регрессоров.
\item Шпионка Шапокляк пытается смоделировать ежедневный выпуск танков, $y$. Выразите оценки коэффициентов регрессии $y = \beta_1 + \beta_2 g +\beta_3 h +\beta_4 x+\varepsilon$ через оценки коэффициентов регрессии на главные компоненты, объясняющие не менее 70\% общей выборочной дисперсии.
\end{enumerate}

\begin{sol}
\end{sol}
\end{problem}


\newpage
\section{На природу! В лес! К деревьям!}

\begin{problem}
	Для случайных величин  $X$ и $Y$ найдите индекс Джини, энтропию и спутанность (perplexity):

\begin{tabular}{lrr}
\toprule
$x$ & $0$ & $1$ \\
$\P(X=x)$ & $0.2$ & $0.8$ \\
\bottomrule
\end{tabular},
\begin{tabular}{lrrr}
\toprule
$y$ & $0$ & $1$ & $5$ \\
$\P(Y=y)$ & $0.2$ & $0.3$ & $0.5$ \\
\bottomrule
\end{tabular}


\begin{sol}
$I_X = 1 - 0.2^2 - 0.8^2 = 0.32$, $H(X) = -(0.2 \ln 0.2 + 0.8 \ln 0.8) \approx 0.5$

$I_Y = 1 - 0.2^2 - 0.3^2 - 0.5^2 = 0.62$, $H(Y) = -(0.2 \ln 0.2 + 0.3 \ln 0.3 + 0.5 \ln 0.5) \approx 1.03$
\end{sol}
\end{problem}


\begin{problem}
Найдите энтропию $X$, спутанность (perplexity) $X$, индекс Джини $X$, если
\begin{enumerate}
  \item величина $X$ равновероятно принимает значения $1$, $7$ и $9$;
  \item величина $X$ равновероятно принимает $k\geq 2$ значений;
  \item величина $X$ равномерно распределена на отрезке $[0;a]$;
  \item величина $X$ нормальна $\cN(\mu;\sigma^2)$;
\end{enumerate}

\begin{sol}
  \begin{enumerate}
  \item $H(X) = \ln 3$, $I_X = 2/3$, спутанность равна $3$.
  \item $I_X = 1-\frac{1}{k}$, $H(X) = \ln k$, спутанность равна $k$.
  \item Если величина $X$ равновероятно принимает $k$ значений, то спутанность равна $k$. У равномерной на $[0;a]$ спутанность равна $a$. $H(X) = -\int_0^a \frac{1}{a} \cdot \ln \frac{1}{a} dx = \ln a$.
  \item Обозначим $f(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)$, тогда $H(X) = -\int_{-\infty}^{+\infty} f(x) \ln (f(x)) dx$.
 \end{enumerate}
\end{sol}
\end{problem}


\begin{problem}
    У Васи была дискретная случайная величина $X$, принимавшая натуральные значения. Вася решил изменить закон распределения величины $X$. Он увеличил количество возможных значений величины $X$ в два раза, разделив каждое событие $X=k$ на два равновероятных подсобытия: $X=k-0.1$ и $X=k+0.1$. Как при этом изменились энтропия, спутанность (perplexity) и индекс Джини?
\begin{sol}
\end{sol}
\end{problem}







\begin{problem}
Случайная величина $X$ принимает значение $1$ с вероятностью $p$ и значение $0$ с вероятностью $1-p$.
\begin{enumerate}
\item Постройте график зависимости индекса Джини и энтропии от $p$.
\item Являются ли функции монотонными? выпуклыми?
\item При каком $p$ энтропия и индекс Джини будут максимальны?
\end{enumerate}


\begin{sol}
$I = 2p(1-p)$, энтропия и индекс Джини максимальны при $p=0.5$.
\end{sol}
\end{problem}



\begin{problem}
Шаман Ыуыуыуыыыыы по прошлым наблюдениям знает, большая охота на мамонта оказывается удачной с вероятностью $0.3$. Если племя ждёт от Ыуыуыуыыыыы прогноз охоты, то Ыуыуыуыыыыы поплясав вокруг костра (10 минут) и постуча бубном (16 раз) прогнозирует удачную охоту с вероятностью $0.3$ и неудачную с вероятностью $0.7$. Конкурирующий шаман Уыуыуыууууу всегда прогнозирует неудачную охоту, как более вероятную. Когда шаман даёт неверный прогноз, его бьют палками.

\begin{enumerate}
  \item Какова вероятность того, что Ыуыуыуыыыыы ошибётся?
  \item Кто чаще бывает бит палками, Ыуыуыуыыыыы или Уыуыуыууууу?
  \item Чему равен индекс Джини для случайной величины равной удаче с вероятностью $0.3$ и неудаче с вероятностью $0.7$?
\end{enumerate}

\begin{sol}
  \[
  I = 2 \cdot 0.7 \cdot 0.3
  \]
\end{sol}
\end{problem}


\begin{problem}
Шаман Ыуыуыуыыыыы заметил по прошлым данным, что в дождливые дни большая охота на мамонта удачна с вероятностью $0.7$, а в сухие — с вероятностью $0.1$. Поэтому в дождливый день Ыуыуыуыыыыы предскажет удачу с вероятностью $0.7$, а в сухой — с вероятностью $0.1$. Дождливых дней — 20\%.

\begin{enumerate}
  \item Какова вероятность того, что Ыуыуыуыыыыы ошибётся?
  \item Чему равен индекс Джини выборки разделённой на две части: в части А шесть бананов и 14 апельсинов, а в части B — восемь бананов и 72 апельсина?
\end{enumerate}

\begin{sol}
\[
I = 0.2 I_L + 0.8 I_R
\]
\end{sol}
\end{problem}


\begin{problem}
Постройте регрессионное дерево для прогнозирования $y$ с помощью $x$ на обучающей выборке:

\begin{tabular}{lrrrr}
\toprule
$x_i$ & $0$ & $1$ & $2$ & $3$ \\
$y_i$ & $5$ & $6$ & $4$ & $100$ \\
\bottomrule
\end{tabular}

Критерий деления узла на два — минимизация $RSS$. Дерево строится до трёх терминальных узлов.


\begin{sol}
Первое разбиение по порогу $x_i < 2.5$, второе — по $x_i < 1.5$.
\end{sol}
\end{problem}

\begin{problem}
Постройте регрессионное дерево для прогнозирования $y$ с помощью $x$ на обучающей выборке:

\begin{tabular}{cc}
\toprule
$y_i$ & $x_i$ \\
\midrule
$100$ & $1$ \\
$102$ & $2$ \\
$103$ & $3$ \\
$50$ & $4$ \\
$55$ & $5$ \\
$61$ & $6$ \\
$70$ & $7$ \\
\bottomrule
\end{tabular}

Критерий деления узла на два — минимизация $RSS$. Узлы делятся до тех пор, пока в узле остаётся больше двух наблюдений.
\begin{sol}
Первое разбинение по порогу $x_i < 3.5$. Левый лист разбивается по порогу $x_i < 5.5$, правый — по порогу $x_i < 1.5$.
\end{sol}
\end{problem}




\begin{problem}
Дон-Жуан предпочитает брюнеток. Перед Новым Годом он посчитал, что в записной книжке у него 20 блондинок, 40 брюнеток, две рыжих и восемь шатенок. С Нового Года Дон-Жуан решил перенести все сведения в две записные книжки, в одну — брюнеток, во вторую — остальных.

Как изменились индекс Джини и энтропия в результате такого разбиения?
\begin{sol}
Было: $I = 1 - \left(\frac{20}{70}\right)^2 - \left(\frac{40}{70}\right)^2- \left(\frac{2}{70}\right)^2- \left(\frac{8}{70}\right)^2 = \frac{708}{1225} \approx 0.58$,

$H =-\left( \frac{20}{70} \ln \frac{20}{70} +  \frac{40}{70} \ln \frac{40}{70} +  \frac{2}{70} \ln \frac{2}{70} +  \frac{8}{70} \ln \frac{8}{70}  \right) \approx 1.03$.

Стало: $I_L = 0$, $I_R = 1 - \left(\frac{20}{30}\right)^2 - \left(\frac{2}{30}\right)^2 - \left(\frac{8}{30}\right)^2 = 0.48$, $I = \frac{40}{70}\cdot 0 + \frac{30}{70} \cdot 0.48 \approx 0.21$,

$H_L = 0$, $H_R = -\left(\frac{20}{30} \ln \frac{20}{30} + \frac{2}{30} \ln \frac{2}{30} + \frac{8}{30} \ln \frac{8}{30} \right) \approx 0.8$, $H =  \frac{40}{70}\cdot 0 + \frac{30}{70} \cdot 0.8 \approx 0.34$.
\end{sol}
\end{problem}



\begin{problem}
Машка пять дней подряд гадала на ромашке, а затем выкладывала очередную фотку «Машка с ромашкой» в инстаграмчик. Результат гадания — переменная $y_i$, количество лайков у фотки — переменная $x_i$. Постройте классификационное дерево для прогнозирования $y_i$ с помощью $x_i$ на обучающей выборке:

\begin{tabular}{cc}
$y_i$ & $x_i$ \\
\hline
плюнет & $10$ \\
поцелует & $11$ \\
поцелует & $12$ \\
к сердцу прижмёт & $13$ \\
к сердцу прижмёт & $14$ \\
\end{tabular}

Дерево строится до идеальной классификации. Критерий деления узла на два — максимальное падение индекса Джини.

\begin{sol}
Первое разбиение по порогу $x_i < 12.5$, второе — по порогу $x_i < 10.5$.
\end{sol}
\end{problem}






\begin{problem}
По данной диаграмме рассеяния постройте классификационное дерево для зависимой переменной $y$:


\begin{minipage}{0.5\textwidth}
\begin{center}
\begin{tikzpicture}[scale = 0.010]
\input{images/tree_scatter_data.tikz}
\end{tikzpicture}
\end{center}
\end{minipage}

Дерево необходимо построить до идеальной классификации, в качестве критерия деления узла на два используйте минимизацию индекса Джини.

\begin{sol}
Сначала делим по $z$, потом по $x$, так как индекс Джини в таком порядке падает сильнее.
\end{sol}
\end{problem}


\begin{problem}
Рассмотрим обучающую выборку для прогнозирования $y$ с помощью $x$ и $z$:

\begin{tabular}{ccc}
\toprule
$y_i$ & $x_i$ & $z_i$ \\
\midrule
$y_1$ & $1$ & $2$ \\
$y_2$ & $1$ & $2$ \\
$y_3$ & $2$ & $2$ \\
$y_4$ & $2$ & $1$\\
$y_5$ & $2$ & $1$ \\
$y_6$ & $2$ & $1$ \\
$y_7$ & $2$ & $1$ \\
\bottomrule
\end{tabular}

Будем называть деревья разными, если они выдают разные прогнозы на обучающей выборке.
Сколько существует разных классификационных деревьев  для данного набора данных?
\begin{sol}

\end{sol}
\end{problem}

\begin{problem}
Исследовательница Мишель строит классификационное дерево для бинарной переменной $y_i$. Может ли при разбиении узла на два расти индекс Джини? Энтропия?
\begin{sol}
Нет, в силу выпуклости функций.
\end{sol}
\end{problem}

\begin{problem}
Приведите примеры наборов данных, для которых индекс Джини равен $0$, $0.5$ и $0.999$.
\begin{sol}
Все $y_i$ одинаковые; поровну $y_i$ двух типов; 1000 разных типов $y_i$, по одному наблюдению каждого типа.
\end{sol}
\end{problem}


\begin{problem}
Рассмотрим задачу построения классификационного дерева для бинарной переменной $y_i$. Приведите пример такого набора данных, что никакое разбиения стартового узла на два не снижает индекс Джини, однако двух разбиений достаточно, чтобы снизить индекс Джини до нуля.
\begin{sol}
\begin{tabular}{ccc}
\toprule
$y_i$ & $x_i$ & $z_i$ \\
\midrule
$1$ & $1$ & $1$ \\
$1$ & $2$ & $2$ \\
$0$ & $1$ & $2$ \\
$0$ & $2$ & $1$\\
\bottomrule
\end{tabular}
\end{sol}
\end{problem}


\begin{problem}
Пятачок собрал данные о визитах Винни-Пуха в гости к Кролику. Здесь $x_i$ — количество съеденого мёда в горшках, а $y_i$ — бинарная переменная, отражающая застревание Винни-Пуха при выходе.

Для построения предиктивной модели Пятачок собирается использовать дерево с заданной структурой:

\begin{minipage}{\textwidth}
\begin{minipage}[t]{0.4\textwidth}
\centering
\begin{tabular}{cc}
\toprule
$y_i$ & $x_i$ \\
\midrule
0 & 1 \\
1 & 4 \\
1 & 2 \\
0 & 3 \\
1 & 3 \\
0 & 1 \\
\bottomrule
\end{tabular}
\end{minipage}
\hfill
\begin{minipage}{0.4\textwidth}
\begin{forest}
  [$x>3.5$, circle, draw
    [$w_L$, circle, draw]
    [$x>2.5$, circle, draw
       [$w_{RL}$, circle, draw]
       [$w_{RR}$, circle, draw]
    ]
  ]
\end{forest}
\end{minipage}
\end{minipage}

Пятачок использует квадратичную аппроксимацию для логистической функции потерь:
\[
Obj(w) = \sum_{i=1}^n \left(loss(y_i, 0) +
loss'_{w}(y_i, 0) (w_i - 0) +
\frac{1}{2} loss''_{ww}(y_i, 0) (w_i - 0)^2 \right) + \frac{1}{2} \lambda |w|^2.
\]


Помогите Очень Маленькому Существу подобрать оптимальные веса $(w_i)$ при $\lambda = 1$.
\begin{sol}
\end{sol}
\end{problem}




\begin{problem}
Нарисовано дерево: деление 1, справа от первого деления — деление 2. Веса равны $w_L$, $w_{RL}$, $w_{LL}$. Дана выборка.

\begin{enumerate}
\item Выпишите в явном виде функцию правдоподобия и логистическую функцию потерь.
\item Оцените $w$ методом максимального правдоподобия.
\item Тут другую функцию потерь написать!
\item Разложите функцию потерь в окрестности $w = (0, 0, 1)$ в ряд Тейлора до второго члена и примерно оцените $w$.
\end{enumerate}

\begin{sol}
\end{sol}
\end{problem}



\section{Бэггинг}


\begin{problem}
У Винни-Пуха есть 100 песенок (кричалок, вопелок, пыхтелок и сопелок). Каждый день он выбирает и поёт одну из них равновероятно наугад. Одну и ту же песенку он может петь несколько раз. Сколько в среднем песенок оказываются неспетыми за 100 дней?


\begin{sol}
$100\cdot \left(\frac{99}{100} \right)^{100}\approx 100/e \approx 37$
\end{sol}
\end{problem}



\begin{problem}
Вася поймал 3 рыбки, весом в 300, 600 и 1200 граммов. И посчитал среднее арифметическое, $\bar x = 700$.

\begin{enumerate}
\item Найдите закон распределения бутстрэп статистики для $\bar x$.
\item Найдите математическое ожидание и дисперсию бутстрэп статистики для $\bar x$.
\item Найдите закон распределения бутстрэп статистики для максимума и минимума для данной выборки.
\end{enumerate}
\begin{sol}
\end{sol}
\end{problem}



\subsection{Разложение на шум-смещение-разброс}


\begin{problem}
Истинная зависимость имеет вид $y_i = x_i^2 + u_i$. Величины $x_i$ независимы и равновероятно принимают значения $1$ и $2$. Величины $u_i$ независимы и равновероятно принимают значения $-1$ и $1$. Начинающий машин-лёрнер Василий может позволить себе обучающую выборку только из двух наблюдений.

Разложите ожидание квадрата ошибки прогноза на шум, смещение и разброс, если:
\begin{enumerate}
	\item Вне зависимости от обучающей выборки из-за ошибки в коде в качестве прогноза всегда выдаётся 0.
	\item Вне зависимости от обучающей выборки из-за ошибки в коде в качестве прогноза равновероятно выдаётся -1 или 1.
	\item По обучающей выборке Василий строит регрессию на константу.
	\item В качестве прогноза разработанный Василием новейший алгоритм всегда выдаёт последний $y$ из обучающей выборки.
	\item По обучающей выборке Василий строит регрессионное дерево минимизируя сумму квадратов остатков.
\end{enumerate}

\begin{sol}
\end{sol}
\end{problem}




\begin{problem}
Истинная зависимость имеет вид $y_i = 3x_i^2 + u_i$. Величины $x_i$ независимы и равновероятно принимаю значения $0$, $1$, $2$. Величины $u_i$ независимы и равновероятно принимают значения $-1$ и $1$.

Исследователь Анатолий оценивает модель линейной регрессии $y_i = \hat \beta x_i $ с помощью МНК.

Разложите ожидание квадрата ошибки прогноза на шум, смещение и разброс.
\begin{sol}
\end{sol}
\end{problem}

\subsection{Случайные проекции}

\begin{problem}
Василий любит сочинять. Особенно он любит сочинять вектора в пространствах большой размерности $n$.  Каждую компоненту каждого вектора он сочиняет по следующему принципу:
\[
z \sim \begin{cases}
-\frac{1}{\sqrt{a}}, \text{ с вероятностью } a^2; \\
0, \text{ с вероятностью } 2(1-a)a; \\
\frac{1}{\sqrt{a}}, \text{ с вероятностью } (1-a)^2; \\
\end{cases},
\]
где $a$ — некоторый параметр.

\begin{enumerate}
  \item Найдите предел по вероятности квадрата длины вектора делённого на размерность пространства.
  \item Найдите предел по вероятности косинуса угла между двумя векторами.
\end{enumerate}

\begin{sol}
\end{sol}
\end{problem}



\Closesolutionfile{solution_file}


% для гиперссылок на условия
% http://tex.stackexchange.com/questions/45415
\renewenvironment{solution}[1]{%
         % add some glue
         \vskip .5cm plus 2cm minus 0.1cm%
         {\bfseries \hyperlink{problem:#1}{#1.}}%
}%
{%
}%

\section{Решения}
\input{all_solutions}

\section{Источники мудрости}
\printbibliography[heading=none]


\end{document}
